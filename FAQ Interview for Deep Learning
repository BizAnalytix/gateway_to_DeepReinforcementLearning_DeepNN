A.  Why GRU is better than LSTM?
Ans- The core concept of LSTM’s are the cell state/ like memory, and it’s various gates. To review, the Forget gate decides what is relevant to keep from prior steps. The input gate decides what information is relevant to add from the current step. The output gate determines what the next hidden state should be. 
GRU’s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.The update gate acts similar to the forget and input gate of an LSTM. It decides what information to throw away and what new information to add. The reset gate is another gate is used to decide how much past information to forget.
And that’s a GRU. GRU’s has fewer tensor operations; therefore, they are a little speedier to train then LSTM’s.

Reference- https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21


Memory

The main benefits of using NumPy arrays should be smaller memory consumption and better runtime behavior. 

For Python Lists -  We can conclude from this that for every new element, we need another eight bytes for the reference to the new object. The new integer object itself consumes 28 bytes. The size of a list "lst" without the size of the elements can be calculated with:

64 + 8 * len(lst) + + len(lst) * 28

NumPy takes up less space. This means that an arbitrary integer array of length "n" in numpy needs

96 + n * 8 Bytes

3. 
The difference between the two TensorFlow codes run for getting the results relies in the way the matrices are generated. In the fastest one, I asked TensorFlow to generate the curves, and so this happens in the GPU; in the slowest one I passed to TensorFlow already existing matrices.

So here we loose lots of time: in the copy of the matrices from the system memory to the GPU one.
https://towardsdatascience.com/numpy-vs-tensorflow-speed-on-matrix-calculations-9cbff6b3ce04

4.
The most obvious differences between NumPy arrays and TensorFlow Tensors are:

Tensors can be backed by accelerator memory (like GPU, TPU).
Tensors are immutable.

5.
Using Eager Execution
When you enable eager execution, operations execute immediately and return their 
values to Python without requiring a Session.run(). For example, to multiply two matrices together, we write this:

Here, the use of the tf.constant(12) Tensor object will promote all math operations to tensor operations, and as such all return values with be tensors.
